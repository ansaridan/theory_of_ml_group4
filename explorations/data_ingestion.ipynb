{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change CWD to repo base for imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "notebook_path = Path().resolve()\n",
    "parent_directory = notebook_path.parent\n",
    "os.chdir(parent_directory)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "import logging\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "# Set up logger\n",
    "log = logging.getLogger(\"etl\")\n",
    "log.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()  # This will print to the console\n",
    "handler.setLevel(logging.INFO)\n",
    "log.addHandler(handler)\n",
    "\n",
    "DATA_PATH = Path() / \"data\"\n",
    "AIRLINE_CODES_FILENAME = \"airline_codes_map.csv\"\n",
    "AIRPORT_CODES_FILENAME = \"airport_id_map.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ingest DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading data/202312_flights.csv\n",
      "reading data/202312_flights.csv\n",
      "reading data/202307_flights.csv\n",
      "reading data/202307_flights.csv\n",
      "reading data/202402_flights.csv\n",
      "reading data/202402_flights.csv\n",
      "reading data/202308_flights.csv\n",
      "reading data/202308_flights.csv\n",
      "reading data/202404_flights.csv\n",
      "reading data/202404_flights.csv\n",
      "reading data/202401_flights.csv\n",
      "reading data/202401_flights.csv\n",
      "reading data/202311_flights.csv\n",
      "reading data/202311_flights.csv\n",
      "reading data/202406_flights.csv\n",
      "reading data/202406_flights.csv\n",
      "reading data/202403_flights.csv\n",
      "reading data/202403_flights.csv\n",
      "skipped data/airport_id_map.csv\n",
      "skipped data/airport_id_map.csv\n",
      "reading data/202310_flights.csv\n",
      "reading data/202310_flights.csv\n",
      "skipped data/airline_codes_map.csv\n",
      "skipped data/airline_codes_map.csv\n",
      "reading data/202405_flights.csv\n",
      "reading data/202405_flights.csv\n",
      "reading data/202309_flights.csv\n",
      "reading data/202309_flights.csv\n"
     ]
    }
   ],
   "source": [
    "SELECTED_COLS = [\n",
    "    # flight identifiers / general data\n",
    "    \"FlightDate\", \"Tail_Number\", \n",
    "    # \"Flight_Number_Reporting_Airline\",\n",
    "    # \"Flights\", \n",
    "    \"Distance\", # \"DistanceGroup\",\n",
    "    #\"FirstDepTime\", \"TotalAddGTime\", \"LongestAddGTime\",\n",
    "    # departure, arrival time\n",
    "    \"CRSDepTime\", \"DepTime\", \"DepTimeBlk\",\n",
    "    \"CRSArrTime\", \"ArrTime\", \"ArrTimeBlk\",\n",
    "    \"ActualElapsedTime\", # \"AirTime\", \n",
    "    # airline identifiers\n",
    "    \"Reporting_Airline\", # \"DOT_ID_Reporting_Airline\", \"IATA_CODE_Reporting_Airline\",\n",
    "    # origin\n",
    "    \"OriginAirportID\", \"OriginCityName\", # \"OriginAirportSeqID\", \"OriginCityMarketID\", \"Origin\", \"OriginCityName\", \"OriginState\", \"OriginStateFips\", \"OriginStateName\", \"OriginWac\",\n",
    "    # destination\n",
    "    \"DestAirportID\", \"DestCityName\", # \"DestAirportSeqID\", \"DestCityMarketID\", \"Dest\", \"DestCityName\", \"DestState\", \"DestStateFips\", \"DestStateName\", \"DestWac\",\n",
    "    # delay data\n",
    "    \"DepDelay\",\"DepDelayMinutes\",\"DepDel15\",\"DepartureDelayGroups\",\n",
    "    \"ArrDelay\",\"ArrDelayMinutes\",\"ArrDel15\", \"ArrivalDelayGroups\",\n",
    "    \"CarrierDelay\",\"WeatherDelay\",\"NASDelay\",\"SecurityDelay\",\"LateAircraftDelay\",\n",
    "    # time spent data\n",
    "    # \"TaxiOut\", \"WheelsOff\", \"WheelsOn\", \"TaxiIn\",\n",
    "    # cancellation data\n",
    "    \"Cancelled\", \"CancellationCode\", \n",
    "    # # diversion data\n",
    "    # \"Diverted\", \"DivAirportLandings\", \"DivReachedDest\", \"DivActualElapsedTime\", \"DivArrDelay\", \"DivDistance\",\n",
    "    # \"Div1Airport\",\"Div1AirportID\",\"Div1AirportSeqID\",\"Div1WheelsOn\",\"Div1TotalGTime\",\"Div1LongestGTime\",\"Div1WheelsOff\",\n",
    "    # \"Div1TailNum\",\"Div2Airport\",\"Div2AirportID\",\"Div2AirportSeqID\",\"Div2WheelsOn\",\"Div2TotalGTime\",\"Div2LongestGTime\",\"Div2WheelsOff\",\n",
    "    # \"Div2TailNum\",\"Div3Airport\",\"Div3AirportID\",\"Div3AirportSeqID\",\"Div3WheelsOn\",\"Div3TotalGTime\",\"Div3LongestGTime\",\"Div3WheelsOff\",\n",
    "    # \"Div3TailNum\",\"Div4Airport\",\"Div4AirportID\",\"Div4AirportSeqID\",\"Div4WheelsOn\",\"Div4TotalGTime\",\"Div4LongestGTime\",\"Div4WheelsOff\",\n",
    "    # \"Div4TailNum\",\"Div5Airport\",\"Div5AirportID\",\"Div5AirportSeqID\",\"Div5WheelsOn\",\"Div5TotalGTime\",\"Div5LongestGTime\",\"Div5WheelsOff\",\"Div5TailNum\",\n",
    "]\n",
    "\n",
    "def calculate_time_difference(start_time, end_time):\n",
    "    if pd.isnull(start_time) or pd.isnull(end_time):\n",
    "        return np.nan\n",
    "    ARBITRATY_DATE = dt.date(1900, 1, 1)\n",
    "    start_dt = dt.datetime.combine(ARBITRATY_DATE, start_time)\n",
    "    end_dt = dt.datetime.combine(ARBITRATY_DATE, end_time)\n",
    "    if end_dt < start_dt:\n",
    "        end_dt += dt.timedelta(days=1)\n",
    "    return (end_dt - start_dt).total_seconds() / 60\n",
    "\n",
    "def _ingest_data() -> pd.DataFrame:\n",
    "    \"\"\"Ingests the data from the CSV files and returns a DataFrame.\"\"\"\n",
    "\n",
    "    # Define the regex pattern for the filenames\n",
    "    pattern = re.compile(r\"^.*flights.*$\")\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    for file in DATA_PATH.glob(\"*.csv\"):\n",
    "        if pattern.match(file.name):\n",
    "            log.info(f\"reading {file}\")\n",
    "            # Read the CSV and append to the list\n",
    "            df_month = pd.read_csv(file, low_memory=False)\n",
    "            dataframes.append(df_month[SELECTED_COLS])\n",
    "        else:\n",
    "            log.info(f\"skipped {file}\")\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    if dataframes:\n",
    "        df_raw = pd.concat(dataframes, ignore_index=True)\n",
    "    else:\n",
    "        df_raw = pd.DataFrame()  # Empty DataFrame if no matching files found\n",
    "    \n",
    "    return df_raw\n",
    "\n",
    "df_raw = _ingest_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Format Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _enrich_data(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Adds additional data from other files to the flight data and formats the data.\"\"\"\n",
    "    \n",
    "    df = df_raw.copy()\n",
    "    # # format date columns\n",
    "    for date_col in [\"FlightDate\"]:\n",
    "        df[date_col] = pd.to_datetime(df[date_col], format=\"%Y-%m-%d\")\n",
    "    for military_time_col in [\"CRSDepTime\", \"DepTime\", \"CRSArrTime\", \"ArrTime\"]:\n",
    "        df[military_time_col] = pd.to_datetime(df[military_time_col].astype(str).str.zfill(4), format=\"%H%M\", errors=\"coerce\").dt.time\n",
    "    df['ScheduledDurationMinutes'] = df.apply(lambda row: calculate_time_difference(row['CRSDepTime'], row['CRSArrTime']), axis=1)\n",
    "    df[\"day_of_week\"] = df[\"FlightDate\"].dt.dayofweek.astype(str) + \"_\" + df[\"FlightDate\"].dt.day_name()\n",
    "    df[\"hour_of_day\"] = [time.hour for time in df[\"CRSDepTime\"]]\n",
    "\n",
    "    # map cancellation reasons\n",
    "    cancellation_code_map = dict(\n",
    "        A=\"Carrier Caused\",\n",
    "        B=\"Weather\",\n",
    "        C=\"National Aviation System\",\n",
    "        D=\"Security\",\n",
    "    )\n",
    "    df.CancellationCode = df.CancellationCode.map(cancellation_code_map)\n",
    "\n",
    "    # join in airline names by code\n",
    "    airline_codes_map = pd.read_csv(DATA_PATH / AIRLINE_CODES_FILENAME)\n",
    "    df = df.join(airline_codes_map.set_index(\"Reporting_Airline\"), on=\"Reporting_Airline\")\n",
    "\n",
    "    # join in airport names by code\n",
    "    # https://www.transtats.bts.gov/FieldInfo.asp?Svryq_Qr5p=b4vtv0%FDNv42146%FP%FDNv42146%FDVQ.%FDN0%FDvqr06vsvpn6v10%FD07zor4%FDn55vt0rq%FDoB%FDhf%FDQbg%FD61%FDvqr06vsB%FDn%FD70v37r%FDnv42146.%FD%FDh5r%FD6uv5%FDsvryq%FDs14%FDnv42146%FDn0nyB5v5%FDnp4155%FDn%FD4n0tr%FD1s%FDBrn45%FDorpn75r%FDn0%FDnv42146%FDpn0%FDpun0tr%FDv65%FDnv42146%FDp1qr%FDn0q%FDnv42146%FDp1qr5%FDpn0%FDor%FD4r75rq.&Svryq_gB2r=a7z&Y11x72_gnoyr=Y_NVecbeg_VQ&gnoyr_VQ=FMF&flf_gnoyr_anzr=g_gEDD_ZNeXRg_NYY_PNeeVRe&fB5_Svryq_anzr=beVTVa_NVecbeg_VQ\n",
    "    airport_id_map = pd.read_csv(DATA_PATH / AIRPORT_CODES_FILENAME)\n",
    "    df = df.join(airport_id_map.set_index(\"Code\").rename(columns={col:f\"Origin{col.replace(\"_\", \"\")}\" for col in airport_id_map.columns if col != \"Code\"}), on=\"OriginAirportID\")\n",
    "    df = df.join(airport_id_map.set_index(\"Code\").rename(columns={col:f\"Dest{col.replace(\"_\", \"\")}\" for col in airport_id_map.columns if col != \"Code\"}), on=\"DestAirportID\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df = _enrich_data(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Miscellaneous Processing**\n",
    "\n",
    "Parse airport map CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the city\n",
    "def extract_city(location):\n",
    "    match = re.search(r\"^(.+?),\", location)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "# Function to extract the state\n",
    "def extract_state(location):\n",
    "    match = re.search(r\",\\s([A-Z]{2})\", location)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Function to extract the airport name\n",
    "def extract_airport_name(location):\n",
    "    match = re.search(r\":\\s(.+)$\", location)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "\n",
    "\n",
    "aiport_id_map = pd.read_csv(DATA_PATH / AIRPORT_CODES_FILENAME)\n",
    "aiport_id_map[\"Airport_City\"] = aiport_id_map[\"Airport_Name\"].apply(extract_city)\n",
    "aiport_id_map[\"Airport_State\"] = aiport_id_map[\"Airport_Name\"].apply(extract_state)\n",
    "aiport_id_map[\"Airport_Short_Name\"] = aiport_id_map[\"Airport_Name\"].apply(extract_airport_name)\n",
    "\n",
    "aiport_id_map.to_csv(\"airport_id_map.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONS_FILENAME = \"state_region_map.csv\"\n",
    "\n",
    "aiport_id_map = pd.read_csv(DATA_PATH / AIRPORT_CODES_FILENAME)\n",
    "regions_map = pd.read_csv(DATA_PATH / REGIONS_FILENAME)\n",
    "\n",
    "pd.merge(aiport_id_map, regions_map[[\"State Code\", \"Region\", \"Division\"]], left_on=\"Airport_State\", right_on=\"State Code\", how=\"left\").drop(columns=\"State Code\").to_csv(\"airport_id_map.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradschool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
